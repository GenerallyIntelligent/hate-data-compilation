import os
import gc

import torch
import optuna

from hatecomp.datasets import MLMA, HASOC, NAACL, NLPCSS, Vicomtech, TwitterSexism
from hatecomp.training import HatecompTrialRunner, HatecompTrialConfig

experiment_root = "experiments"
dataset = NLPCSS()
num_trials = 15

print("Loaded dataset: ", dataset.__name__)

experiment_directory = os.path.join(experiment_root, dataset.__name__)
os.makedirs(experiment_directory, exist_ok=True)

best_so_far = 0.0
best_so_far_location = None


def objective(trial: optuna.trial.Trial):
    transformer_name = trial.suggest_categorical(
        "transformer_name",
        [
            "roberta-base",
            "xlm-roberta-base",
            "distilroberta-base",
        ],
    )
    epochs = trial.suggest_int("epochs", 3, 30)
    training_batch_size = trial.suggest_int("training_batch_size", 8, 32)
    learning_rate = trial.suggest_float("learning_rate", 1e-6, 1e-4, log=True)
    weight_decay = trial.suggest_float("weight_decay", 1e-4, 0.01, log=True)
    learning_rate_warmup_percentage = trial.suggest_float(
        "learning_rate_warmup_percentage", 0.1, 0.4
    )
    head_hidden_size = trial.suggest_int("head_hidden_size", 256, 1024)
    dropout = trial.suggest_float("dropout", 0.001, 0.1, log=True)

    config = HatecompTrialConfig(
        transformer_name=transformer_name,
        epochs=epochs,
        training_batch_size=training_batch_size,
        evaluation_batch_size=64,
        learning_rate=learning_rate,
        weight_decay=weight_decay,
        learning_rate_warmup_percentage=learning_rate_warmup_percentage,
        dropout=dropout,
        head_hidden_size=head_hidden_size,
    )
    trial_runner = HatecompTrialRunner(
        experiment_directory, dataset, config, checkpoint=True, verbose=False
    )
    print("Starting trial: ", config.name)
    result = trial_runner.run()
    del trial_runner
    gc.collect()
    torch.cuda.empty_cache()

    global best_so_far
    global best_so_far_location
    if result > best_so_far:
        if best_so_far_location is not None:
            os.remove(best_so_far_location)
        best_so_far = result
        best_so_far_location = os.path.join(
            experiment_directory, config.name, "best_model.pt"
        )
    else:
        # Delete the best model generated by this trial
        best_model_path = os.path.join(
            experiment_directory, config.name, "best_model.pt"
        )
        os.remove(best_model_path)

    return result


if __name__ == "__main__":
    study = optuna.create_study(direction="maximize")
    study.optimize(objective, n_trials=num_trials)

    pruned_trials = study.get_trials(
        deepcopy=False, states=[optuna.trial.TrialState.PRUNED]
    )
    complete_trials = study.get_trials(
        deepcopy=False, states=[optuna.trial.TrialState.COMPLETE]
    )
    failed_trials = study.get_trials(
        deepcopy=False, states=[optuna.trial.TrialState.FAIL]
    )

    print("Study statistics: ")
    print("  Number of finished trials: ", len(study.trials))
    print("  Number of pruned trials: ", len(pruned_trials))
    print("  Number of complete trials: ", len(complete_trials))
    print("  Number of failed trials: ", len(failed_trials))

    print("Best trial:")
    trial = study.best_trial

    print("  Value: ", trial.value)

    print("  Params: ")
    for key, value in trial.params.items():
        print("    {}: {}".format(key, value))
